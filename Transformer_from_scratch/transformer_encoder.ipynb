{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZOlKWZt7ziGS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LR-aZ2Eozn_u"
      },
      "outputs": [],
      "source": [
        "#rope embedding\n",
        "class RoPEEmbedding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        assert embedding_dim % 2 == 0, \"Embedding dimension must be even for RoPE\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Rotary Position Embedding.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Tensor with RoPE applied to the last two dimensions.\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Generate position indices\n",
        "        position_ids = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
        "\n",
        "        # Compute the rotary angles\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, self.embedding_dim, 2, dtype=torch.float32, device=x.device) / self.embedding_dim))\n",
        "        angles = torch.einsum('i,j->ij', position_ids, freqs)\n",
        "\n",
        "        # Create the rotation matrix for sin and cos embeddings\n",
        "        sin = torch.sin(angles).repeat_interleave(2, dim=-1)\n",
        "        cos = torch.cos(angles).repeat_interleave(2, dim=-1)\n",
        "\n",
        "        # Apply rotation using cos and sin embeddings\n",
        "        x1 = x * cos + self.rotate_half(x) * sin\n",
        "        return x1\n",
        "\n",
        "\n",
        "    def rotate_half(self,x):\n",
        "        \"\"\"\n",
        "        Rotate the last dimension of the input tensor by swapping odd and even elements and negating one.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (..., embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Rotated tensor of the same shape.\n",
        "        \"\"\"\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split into even and odd dimensions\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "#sine embedding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Generate the positional encoding\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "        # Ensure the PE matches the batch size and sequence length\n",
        "        PE = PE.unsqueeze(0).expand(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return PE + x\n",
        "\n",
        "\n",
        "#feedforward network\n",
        "class feedforward(nn.Module):\n",
        "  def __init__(self,d_model,hidlayer,dropout):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.hidlayer=hidlayer\n",
        "    self.linearlayer1=nn.Linear(self.d_model,self.hidlayer)\n",
        "    self.linearlayer2=nn.Linear(self.hidlayer,self.d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.activation=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    l1=self.linearlayer1(x)\n",
        "    print(f\"x after first linear layer: {x.size()}\")\n",
        "    l1=self.activation(l1)\n",
        "    print(f\"x after activation: {l1.size()}\")\n",
        "    l1=self.dropout(l1)\n",
        "    print(f\"x after dropout 1: {l1.size()}\")\n",
        "    out=self.linearlayer2(l1)\n",
        "    print(f\"x after 2nd linear layer: {out.size()}\")\n",
        "    #drop out gen not aplpied after 1st layhers\n",
        "    out=self.dropout(out)\n",
        "    print(f\"x after dropout 2: {out.size()}\")\n",
        "    return out\n",
        "\n",
        "#multhead attention\n",
        "class multihead_attention(nn.Module):\n",
        "    def __init__(self, inputdim, dmodel, masking=None, heads=1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.masking = masking\n",
        "        assert dmodel % heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
        "        self.inputdim = inputdim\n",
        "        self.dmodel = dmodel\n",
        "        self.head_dim = self.dmodel // self.heads\n",
        "        self.wq = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.wk = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.wv = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.linearlayer=nn.Linear(self.dmodel,self.dmodel)\n",
        "        self.projectionlayer=nn.Linear(self.dmodel,self.inputdim)\n",
        "        print('heads =', self.heads)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        dk = torch.tensor(q.shape[-1], dtype=torch.float32)\n",
        "        scaled = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(dk)\n",
        "\n",
        "        if self.masking is not None:\n",
        "            mask = torch.ones(q.shape[2], q.shape[2], device=q.device)\n",
        "            mask = torch.tril(mask)\n",
        "            mask[mask == 0] = -torch.inf\n",
        "            mask[mask == 1] = 0\n",
        "            scaled = scaled + mask\n",
        "\n",
        "        attention = torch.softmax(scaled, dim=-1)\n",
        "        scores = torch.matmul(attention, v)\n",
        "        return attention, scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        q = self.wq(x)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "        q = q.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        k = k.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        v = v.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attention, scores = self.scaled_dot_product_attention(q, k, v)\n",
        "        #print('scores init',scores.shape)\n",
        "        scores = scores.reshape(batch_size, sequence_length, self.heads *self.head_dim)# we can use self.dmodel as well as last arg\n",
        "        #print('scores shape',scores.shape)\n",
        "        out=self.linearlayer(scores)\n",
        "        #print('out',out.shape)\n",
        "        projected=self.projectionlayer(out)\n",
        "        #print('projected shape',projected.shape)\n",
        "        #print()\n",
        "        return projected\n",
        "\n",
        "#layer norm\n",
        "class CustomLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        super(CustomLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate mean and std across the last dimension (features) for each sequence in the batch\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x_normalized = (x - mean) / (std + self.epsilon)\n",
        "\n",
        "        # Apply gamma and beta, which are learned parameters for normalization\n",
        "        # The shape of gamma and beta should match the feature size\n",
        "        return self.gamma.unsqueeze(0).unsqueeze(0) * x_normalized + self.beta.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "#encopder layer\n",
        "\n",
        "class encoderlayer(nn.Module):\n",
        "  def __init__(self,input_dim,d_model,hidlayer,dropout,num_heads,masking):\n",
        "    super().__init__()\n",
        "    self.input_dim,self.d_model,self.hidlayer,self.dropout,self.num_heads,self.masking=input_dim,d_model,hidlayer,dropout,num_heads,masking\n",
        "    #self.PositionalEncoding=PositionalEncoding(self.d_model,self.input_dim) #sinencoding\n",
        "    self.rope_embedding=RoPEEmbedding(self.input_dim)\n",
        "    self.multihead_attention=multihead_attention(input_dim,d_model,masking,num_heads)\n",
        "    self.feedforward=feedforward(self.input_dim,self.hidlayer,self.dropout)\n",
        "    self.layernorm=CustomLayerNorm(self.input_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    #rope\n",
        "    print('---positional encoding--')\n",
        "    re=self.rope_embedding(x)\n",
        "    print(re.shape)\n",
        "    #mulihead\n",
        "    print('--mulihead attention--')\n",
        "    mha=self.multihead_attention(re+x)\n",
        "    print(mha.shape)\n",
        "    #layernorm\n",
        "    print('--layer normalisation--')\n",
        "    ln1=self.layernorm(mha+re+x)\n",
        "    print(ln1.shape)\n",
        "    #feedforward\n",
        "    print('--feedforward network--')\n",
        "    ff=self.feedforward(ln1)\n",
        "    print(ff.shape)\n",
        "    #layernorm\n",
        "    print('--layer normalisation--')\n",
        "    out=self.layernorm(ff+ln1)\n",
        "    print(out.shape)\n",
        "    return out\n",
        "\n",
        "class encoder(nn.Module):\n",
        "  #creating n layer of layers\n",
        "  def __init__(self,input_dim,d_model,hidlayer,dropout,num_heads,masking,nlayers):\n",
        "    super().__init__()\n",
        "    #sequentially stack encoders\n",
        "    self.layers=nn.Sequential(*[encoderlayer(input_dim,d_model,hidlayer,dropout,num_heads,masking) for _ in range(nlayers)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      print(f'\\n------layer {i+1}----- ')\n",
        "      x = layer(x)\n",
        "      print(f\"--Output after layer {i+1}--: {x.size()}\")  # Printing the size after each layer\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7pL4FaKMBMf",
        "outputId": "6616c03b-a23a-4cfb-cd3d-8e7b4f1a4433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input x: tensor([[[-0.0766,  0.3599, -0.7820,  0.0715],\n",
            "         [ 0.6648, -0.2868,  1.6206, -1.5967],\n",
            "         [ 0.4046,  0.6113,  0.7604, -0.0336]],\n",
            "\n",
            "        [[-0.3448,  0.4937, -0.0776, -1.8054],\n",
            "         [ 0.4851,  0.2052,  0.3384,  1.3528],\n",
            "         [ 0.3736,  0.0134,  0.7737, -0.1092]]])\n",
            "heads = 2\n",
            "heads = 2\n",
            "\n",
            "------layer 1----- \n",
            "---positional encoding--\n",
            "torch.Size([2, 3, 4])\n",
            "--mulihead attention--\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation--\n",
            "torch.Size([2, 3, 4])\n",
            "--feedforward network--\n",
            "x after first linear layer: torch.Size([2, 3, 4])\n",
            "x after activation: torch.Size([2, 3, 2048])\n",
            "x after dropout 1: torch.Size([2, 3, 2048])\n",
            "x after 2nd linear layer: torch.Size([2, 3, 4])\n",
            "x after dropout 2: torch.Size([2, 3, 4])\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation--\n",
            "torch.Size([2, 3, 4])\n",
            "--Output after layer 1--: torch.Size([2, 3, 4])\n",
            "\n",
            "------layer 2----- \n",
            "---positional encoding--\n",
            "torch.Size([2, 3, 4])\n",
            "--mulihead attention--\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation--\n",
            "torch.Size([2, 3, 4])\n",
            "--feedforward network--\n",
            "x after first linear layer: torch.Size([2, 3, 4])\n",
            "x after activation: torch.Size([2, 3, 2048])\n",
            "x after dropout 1: torch.Size([2, 3, 2048])\n",
            "x after 2nd linear layer: torch.Size([2, 3, 4])\n",
            "x after dropout 2: torch.Size([2, 3, 4])\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation--\n",
            "torch.Size([2, 3, 4])\n",
            "--Output after layer 2--: torch.Size([2, 3, 4])\n",
            "encoder shape torch.Size([2, 3, 4])\n",
            "encoder output: tensor([[[ 0.0867,  1.1103, -1.3208,  0.1237],\n",
            "         [ 0.1328,  0.4871,  0.8199, -1.4397],\n",
            "         [-0.4734, -0.3064,  1.4803, -0.7005]],\n",
            "\n",
            "        [[-0.0777,  0.9432,  0.4972, -1.3627],\n",
            "         [ 0.2020, -0.9619, -0.5542,  1.3141],\n",
            "         [ 0.2994, -0.4000,  1.2219, -1.1213]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#torch.manual_seed(4)\n",
        "input_dim = 4\n",
        "d_model = 512\n",
        "num_heads = 2\n",
        "batch_size = 2\n",
        "sequence_length = 3\n",
        "dropout=0.2\n",
        "hidden=2048\n",
        "layers=2\n",
        "\n",
        "torch.manual_seed(3)\n",
        "x = torch.randn(batch_size, sequence_length, input_dim)\n",
        "print('Input x:', x)\n",
        "Encoder = encoder(input_dim,d_model,hidlayer=hidden,dropout=dropout,num_heads=num_heads,masking=None,nlayers=layers)\n",
        "Encoderout=Encoder(x)\n",
        "\n",
        "print('encoder shape', Encoderout.shape)\n",
        "print(\"encoder output:\", Encoderout)\n",
        "\n",
        "\n",
        "#perfect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjvhObhZXRlN",
        "outputId": "06f7d626-f4ee-494f-a44f-20a25b8830a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0354, -1.9893],\n",
              "        [-0.3161, -0.8495]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a=nn.Embedding(3,2)\n",
        "y=a(torch.tensor([1,2]))\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26kHr65obtrO",
        "outputId": "f53acd5b-8b6d-4c50-a1d1-6f97eabc1bb0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([4, 2])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.tensor([4,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgHOJwoscCYl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e-cuq55IfqJq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdn_mUlDgAgp",
        "outputId": "e0a5e8ed-d231-4341-d318-fddbd4ea007a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
              "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fnDYiP6hgDcs"
      },
      "outputs": [],
      "source": [
        "#my own\n",
        "#rope embedding\n",
        "class RoPEEmbedding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        assert embedding_dim % 2 == 0, \"Embedding dimension must be even for RoPE\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Rotary Position Embedding.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Tensor with RoPE applied to the last two dimensions.\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Generate position indices\n",
        "        position_ids = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
        "\n",
        "        # Compute the rotary angles\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, self.embedding_dim, 2, dtype=torch.float32, device=x.device) / self.embedding_dim))\n",
        "        angles = torch.einsum('i,j->ij', position_ids, freqs)\n",
        "\n",
        "        # Create the rotation matrix for sin and cos embeddings\n",
        "        sin = torch.sin(angles).repeat_interleave(2, dim=-1)\n",
        "        cos = torch.cos(angles).repeat_interleave(2, dim=-1)\n",
        "\n",
        "        # Apply rotation using cos and sin embeddings\n",
        "        x1 = x * cos + self.rotate_half(x) * sin\n",
        "        return x1\n",
        "\n",
        "\n",
        "    def rotate_half(self,x):\n",
        "        \"\"\"\n",
        "        Rotate the last dimension of the input tensor by swapping odd and even elements and negating one.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (..., embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Rotated tensor of the same shape.\n",
        "        \"\"\"\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split into even and odd dimensions\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "#sine embedding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Generate the positional encoding\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "        # Ensure the PE matches the batch size and sequence length\n",
        "        PE = PE.unsqueeze(0).expand(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return PE + x\n",
        "\n",
        "\n",
        "#feedforward network\n",
        "class feedforward(nn.Module):\n",
        "  def __init__(self,d_model,hidlayer,dropout):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.hidlayer=hidlayer\n",
        "    self.linearlayer1=nn.Linear(self.d_model,self.hidlayer)\n",
        "    self.linearlayer2=nn.Linear(self.hidlayer,self.d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.activation=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    l1=self.linearlayer1(x)\n",
        "    print(f\"x after first linear layer: {x.size()}\")\n",
        "    l1=self.activation(l1)\n",
        "    print(f\"x after activation: {l1.size()}\")\n",
        "    l1=self.dropout(l1)\n",
        "    print(f\"x after dropout 1: {l1.size()}\")\n",
        "    out=self.linearlayer2(l1)\n",
        "    print(f\"x after 2nd linear layer: {out.size()}\")\n",
        "    #drop out gen not aplpied after 1st layhers\n",
        "    out=self.dropout(out)\n",
        "    print(f\"x after dropout 2: {out.size()}\")\n",
        "    return out\n",
        "\n",
        "#multhead attention\n",
        "class multihead_attention(nn.Module):\n",
        "    def __init__(self, inputdim, dmodel,heads=1,masking=None):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.masking = masking\n",
        "        assert dmodel % heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
        "        self.inputdim = inputdim\n",
        "        self.dmodel = dmodel\n",
        "        self.head_dim = self.dmodel // self.heads\n",
        "        self.wq = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.wk = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.wv = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.linearlayer=nn.Linear(self.dmodel,self.dmodel)\n",
        "        self.projectionlayer=nn.Linear(self.dmodel,self.inputdim)\n",
        "        print('heads =', self.heads)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        dk = torch.tensor(q.shape[-1], dtype=torch.float32)\n",
        "        scaled = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(dk)\n",
        "\n",
        "        if self.masking is not None:\n",
        "            mask = torch.ones(q.shape[2], q.shape[2], device=q.device)\n",
        "            mask = torch.tril(mask)\n",
        "            mask[mask == 0] = -torch.inf\n",
        "            mask[mask == 1] = 0\n",
        "            scaled = scaled + mask\n",
        "\n",
        "        attention = torch.softmax(scaled, dim=-1)\n",
        "        scores = torch.matmul(attention, v)\n",
        "        return attention, scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        q = self.wq(x)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "        q = q.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        k = k.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        v = v.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attention, scores = self.scaled_dot_product_attention(q, k, v)\n",
        "        #print('scores init',scores.shape)\n",
        "        scores = scores.reshape(batch_size, sequence_length, self.heads *self.head_dim)# we can use self.dmodel as well as last arg\n",
        "        #print('scores shape',scores.shape)\n",
        "        out=self.linearlayer(scores)\n",
        "        #print('out',out.shape)\n",
        "        projected=self.projectionlayer(out)\n",
        "        #print('projected shape',projected.shape)\n",
        "        #print()\n",
        "        return projected\n",
        "\n",
        "\n",
        "\n",
        "class multihead_cross_attention(nn.Module):\n",
        "    def __init__(self, inputdim, dmodel, masking=None, heads=1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.masking = masking\n",
        "        assert dmodel % heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
        "        self.inputdim = inputdim\n",
        "        self.dmodel = dmodel\n",
        "        self.head_dim = self.dmodel // self.heads\n",
        "        self.wqc = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.wkc = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.wvc = nn.Linear(self.inputdim, self.dmodel)\n",
        "        self.linearlayer=nn.Linear(self.dmodel,self.dmodel)\n",
        "        self.projectionlayer=nn.Linear(self.dmodel,self.inputdim)\n",
        "        #print('heads =', self.heads)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        dk = torch.tensor(q.shape[-1], dtype=torch.float32)\n",
        "        scaled = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(dk)\n",
        "\n",
        "        if self.masking is not None:\n",
        "            mask = torch.ones(q.shape[2], q.shape[2], device=q.device)\n",
        "            mask = torch.tril(mask)\n",
        "            mask[mask == 0] = -torch.inf\n",
        "            mask[mask == 1] = 0\n",
        "            scaled = scaled + mask\n",
        "\n",
        "        attention = torch.softmax(scaled, dim=-1)\n",
        "        scores = torch.matmul(attention, v)\n",
        "        return attention, scores\n",
        "\n",
        "    def forward(self, x,y):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        # q and k from decoder\n",
        "        q = self.wqc(x)\n",
        "        k = self.wkc(x)\n",
        "        # v from decoder\n",
        "        v = self.wvc(y)\n",
        "        q = q.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        k = k.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        v = v.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attention, scores = self.scaled_dot_product_attention(q, k, v)\n",
        "        #print('scores init',scores.shape)\n",
        "        scores = scores.reshape(batch_size, sequence_length, self.heads *self.head_dim)# we can use self.dmodel as well as last arg\n",
        "        #print('scores shape',scores.shape)\n",
        "        out=self.linearlayer(scores)\n",
        "        #print('out',out.shape)\n",
        "        projected=self.projectionlayer(out)\n",
        "        #print('projected shape',projected.shape)\n",
        "        #print()\n",
        "        return projected\n",
        "\n",
        "#layer norm\n",
        "class CustomLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        super(CustomLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate mean and std across the last dimension (features) for each sequence in the batch\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x_normalized = (x - mean) / (std + self.epsilon)\n",
        "\n",
        "        # Apply gamma and beta, which are learned parameters for normalization\n",
        "        # The shape of gamma and beta should match the feature size\n",
        "        return self.gamma.unsqueeze(0).unsqueeze(0) * x_normalized + self.beta.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "#decoder layer\n",
        "\n",
        "class decoderlayer(nn.Module):\n",
        "  def __init__(self,input_dim,d_model,hidlayer,dropout,num_heads,masking):\n",
        "    super().__init__()\n",
        "    self.input_dim,self.d_model,self.hidlayer,self.dropout,self.num_heads,self.masking=input_dim,d_model,hidlayer,dropout,num_heads,masking\n",
        "    #self.PositionalEncoding=PositionalEncoding(self.d_model,self.input_dim) #sinencoding\n",
        "    self.rope_embedding=RoPEEmbedding(self.input_dim)\n",
        "    self.multihead_attention=multihead_attention(input_dim,d_model,num_heads,masking=True)\n",
        "    self.layernorm1=CustomLayerNorm(self.input_dim)\n",
        "    self.cross_attention=multihead_cross_attention(input_dim,d_model,num_heads)\n",
        "    self.layernorm2=CustomLayerNorm(self.input_dim)\n",
        "    self.feedforward=feedforward(self.input_dim,self.hidlayer,self.dropout)\n",
        "    self.layernorm3=CustomLayerNorm(self.input_dim)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x,y):\n",
        "    #rope\n",
        "    print('---positional encoding--')\n",
        "    red=self.rope_embedding(y)\n",
        "    print(red.shape)\n",
        "    #mulihead\n",
        "    print('--mulihead attention--')\n",
        "    mha=self.multihead_attention(red+y)\n",
        "    print(mha.shape)\n",
        "    #layernorm1\n",
        "    print('--layer normalisation 1--')\n",
        "    ln1=self.layernorm1(mha+red+y)\n",
        "    print(ln1.shape)\n",
        "    #cross attention\n",
        "    print('--cross mulihead attention--')\n",
        "    cmha=self.cross_attention(x,ln1)\n",
        "    print(cmha.shape)\n",
        "     #layernorm2\n",
        "    print('--layer normalisation 2--')\n",
        "    ln2=self.layernorm2(cmha+ln1)\n",
        "    print(ln2.shape)\n",
        "\n",
        "    #feedforward\n",
        "    print('--feedforward network--')\n",
        "    ff=self.feedforward(ln1)\n",
        "    print(ff.shape)\n",
        "    #layernorm3\n",
        "    print('--layer normalisation--')\n",
        "    out=self.layernorm3(ff+ln2)\n",
        "    print(out.shape)\n",
        "    return out\n",
        "\n",
        "class decoder(nn.Module):\n",
        "  #creating n layer of layers\n",
        "  def __init__(self,input_dim,d_model,hidlayer,dropout,num_heads,nlayers,masking=None):\n",
        "    super().__init__()\n",
        "    #sequentially stack encoders\n",
        "    self.layers=nn.Sequential(*[decoderlayer(input_dim,d_model,hidlayer,dropout,num_heads,masking) for _ in range(nlayers)])\n",
        "\n",
        "  def forward(self, x,y):\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      print(f'\\n------layer {i+1}----- ')\n",
        "      l = layer(x,y)\n",
        "      print(f\"--Output after layer {i+1}--: {x.size()}\")  # Printing the size after each layer\n",
        "    return l\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLCI6tVInMD3",
        "outputId": "00a2e995-4b34-4fba-801c-20cbc74462a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input x: tensor([[[-0.0766,  0.3599, -0.7820,  0.0715],\n",
            "         [ 0.6648, -0.2868,  1.6206, -1.5967],\n",
            "         [ 0.4046,  0.6113,  0.7604, -0.0336]],\n",
            "\n",
            "        [[-0.3448,  0.4937, -0.0776, -1.8054],\n",
            "         [ 0.4851,  0.2052,  0.3384,  1.3528],\n",
            "         [ 0.3736,  0.0134,  0.7737, -0.1092]]])\n",
            "output Y: tensor([[[-1.1963,  1.0280,  0.0719, -0.1845],\n",
            "         [-1.5159,  0.9125,  0.2539, -0.6924],\n",
            "         [-0.0752, -0.4233,  0.4217, -0.2576]],\n",
            "\n",
            "        [[-1.5835,  1.3960, -1.0319,  1.1391],\n",
            "         [ 0.5125, -0.0198, -1.1216, -0.4891],\n",
            "         [-0.6336, -0.7893, -0.8977, -1.8876]]])\n",
            "heads = 4\n",
            "heads = 4\n",
            "\n",
            "------layer 1----- \n",
            "---positional encoding--\n",
            "torch.Size([2, 3, 4])\n",
            "--mulihead attention--\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation 1--\n",
            "torch.Size([2, 3, 4])\n",
            "--cross mulihead attention--\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation 2--\n",
            "torch.Size([2, 3, 4])\n",
            "--feedforward network--\n",
            "x after first linear layer: torch.Size([2, 3, 4])\n",
            "x after activation: torch.Size([2, 3, 2048])\n",
            "x after dropout 1: torch.Size([2, 3, 2048])\n",
            "x after 2nd linear layer: torch.Size([2, 3, 4])\n",
            "x after dropout 2: torch.Size([2, 3, 4])\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation--\n",
            "torch.Size([2, 3, 4])\n",
            "--Output after layer 1--: torch.Size([2, 3, 4])\n",
            "\n",
            "------layer 2----- \n",
            "---positional encoding--\n",
            "torch.Size([2, 3, 4])\n",
            "--mulihead attention--\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation 1--\n",
            "torch.Size([2, 3, 4])\n",
            "--cross mulihead attention--\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation 2--\n",
            "torch.Size([2, 3, 4])\n",
            "--feedforward network--\n",
            "x after first linear layer: torch.Size([2, 3, 4])\n",
            "x after activation: torch.Size([2, 3, 2048])\n",
            "x after dropout 1: torch.Size([2, 3, 2048])\n",
            "x after 2nd linear layer: torch.Size([2, 3, 4])\n",
            "x after dropout 2: torch.Size([2, 3, 4])\n",
            "torch.Size([2, 3, 4])\n",
            "--layer normalisation--\n",
            "torch.Size([2, 3, 4])\n",
            "--Output after layer 2--: torch.Size([2, 3, 4])\n",
            "encoder shape torch.Size([2, 3, 4])\n",
            "encoder output: tensor([[[-1.3021,  0.7475,  0.8214, -0.2668],\n",
            "         [-1.0329,  0.6705,  1.0232, -0.6608],\n",
            "         [-0.4707,  0.4817,  1.1228, -1.1339]],\n",
            "\n",
            "        [[-0.9895,  0.7549, -0.7264,  0.9610],\n",
            "         [ 0.8215,  0.9067, -0.9341, -0.7941],\n",
            "         [ 0.3292,  1.2028, -0.3976, -1.1344]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "#torch.manual_seed(4)\n",
        "input_dim = 4\n",
        "d_model = 512\n",
        "num_heads = 4\n",
        "batch_size = 2\n",
        "sequence_length = 3\n",
        "dropout=0.2\n",
        "hidden=2048\n",
        "layers=2\n",
        "\n",
        "torch.manual_seed(3)\n",
        "x = torch.randn(batch_size, sequence_length, input_dim)\n",
        "print('Input x:', x)\n",
        "y= torch.randn(batch_size, sequence_length, input_dim)\n",
        "print('output Y:', y)\n",
        "Decoder = decoder(input_dim,d_model,hidlayer=hidden,dropout=dropout,num_heads=num_heads,nlayers=layers)\n",
        "Decoderout=Decoder(x,y)\n",
        "\n",
        "print('encoder shape', Decoderout.shape)\n",
        "print(\"encoder output:\", Decoderout)\n",
        "\n",
        "\n",
        "#perfect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s2KAbq_4nNih"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8LMf2VLtnpo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8QNxqajv-JTj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7GLB2PNKjaBe"
      },
      "outputs": [],
      "source": [
        "#to create custom dataset pytorch inbuilt class used\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, lang_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.lang_sentences = lang_sentences\n",
        "    #rturn number of eng/hind or sentences in a list\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "    #return 1:1 mapping of one lang to other\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.lang_sentences[idx]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = RoPEEmbedding(d_model)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
        "      def tokenize(sentence, start_token=True, end_token=True):\n",
        "          # Convert sentence to list of word indices\n",
        "          sentence_word_indices = [self.language_to_index[token] for token in list(sentence)]\n",
        "\n",
        "          # Add start and end tokens if needed\n",
        "          if start_token:\n",
        "              sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "          if end_token:\n",
        "              sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "\n",
        "          # Pad the sentence to max_sequence_length\n",
        "          while len(sentence_word_indices) < self.max_sequence_length:\n",
        "              sentence_word_indices.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "\n",
        "          # Ensure the sequence length doesn't exceed max_sequence_length\n",
        "          sentence_word_indices = sentence_word_indices[:self.max_sequence_length]\n",
        "\n",
        "          return torch.tensor(sentence_word_indices)\n",
        "\n",
        "      tokenized = []\n",
        "      for sentence_num in range(len(batch)):\n",
        "          tokenized.append(tokenize(batch[sentence_num], start_token, end_token))\n",
        "\n",
        "      # All sentences are now padded to the same length, so stack them\n",
        "      tokenized = torch.stack(tokenized)  # All tensors will have the same size now\n",
        "      return tokenized\n",
        "\n",
        "\n",
        "    def forward(self, x,start_token, end_token=True): # sentence\n",
        "        x = self.batch_tokenize(x ,start_token,end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "iKoo0qUI-oIR"
      },
      "outputs": [],
      "source": [
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        # create embedding of dmodel from input tokens\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = RoPEEmbedding(d_model)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self,batch,start,end):\n",
        "      def tokenize(sentence):\n",
        "        words = sentence.split()  # ✅ Now splits by words\n",
        "        # for below The .get() method of a dictionary tries to find the token in language_to_index.If token exists in the dictionary, it returns the corresponding index.If token does not exist in the dictionary, it returns language_to_index[PADDING_TOKEN],\n",
        "        sentence_word_indices = [self.language_to_index.get(token, self.language_to_index[self.PADDING_TOKEN]) for token in words]\n",
        "        # ✅ Apply truncation BEFORE adding special tokens\n",
        "        sentence_word_indices = sentence_word_indices[:self.max_sequence_length - (1 if start else 0) - (1 if end else 0)]\n",
        "\n",
        "        # ✅ Add special tokens\n",
        "        if start:\n",
        "            sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "\n",
        "        if end:\n",
        "            sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "\n",
        "\n",
        "        # Apply padding if needed (padding should be added after truncation in case after trunction needs padding)\n",
        "        if len(sentence_word_indices) < self.max_sequence_length:\n",
        "            padding_length = self.max_sequence_length - len(sentence_word_indices)\n",
        "            sentence_word_indices.extend([self.language_to_index[self.PADDING_TOKEN]] * padding_length)\n",
        "\n",
        "        return torch.tensor(sentence_word_indices)\n",
        "\n",
        "      return torch.stack([tokenize(sentence) for sentence in batch])\n",
        "\n",
        "\n",
        "    def forward(self, x,start,end): # sentence\n",
        "        x = self.batch_tokenize(x,start,end)\n",
        "        print('input tokenization of x')\n",
        "        print(x.shape)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder(x)\n",
        "        #print('---positional embedding---')\n",
        "        #print(pos.shape)\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "\n",
        "#rope embedding\n",
        "class RoPEEmbedding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        assert embedding_dim % 2 == 0, \"Embedding dimension must be even for RoPE\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Rotary Position Embedding.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Tensor with RoPE applied to the last two dimensions.\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Generate position indices\n",
        "        position_ids = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
        "\n",
        "        # Compute the rotary angles\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, self.embedding_dim, 2, dtype=torch.float32, device=x.device) / self.embedding_dim))\n",
        "        angles = torch.einsum('i,j->ij', position_ids, freqs)\n",
        "\n",
        "        # Create the rotation matrix for sin and cos embeddings\n",
        "        sin = torch.sin(angles).repeat_interleave(2, dim=-1)\n",
        "        cos = torch.cos(angles).repeat_interleave(2, dim=-1)\n",
        "\n",
        "        # Apply rotation using cos and sin embeddings\n",
        "        x1 = x * cos + self.rotate_half(x) * sin\n",
        "        return x1\n",
        "\n",
        "\n",
        "    def rotate_half(self,x):\n",
        "        \"\"\"\n",
        "        Rotate the last dimension of the input tensor by swapping odd and even elements and negating one.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (..., embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Rotated tensor of the same shape.\n",
        "        \"\"\"\n",
        "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split into even and odd dimensions\n",
        "        return torch.cat((-x2, x1), dim=-1)\n",
        "\n",
        "#sine embedding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Generate the positional encoding\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "        # Ensure the PE matches the batch size and sequence length\n",
        "        PE = PE.unsqueeze(0).expand(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return PE + x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CWm55N_OyrMD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#feedforward network\n",
        "class feedforward(nn.Module):\n",
        "  def __init__(self,d_model,hidlayer,dropout):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.hidlayer=hidlayer\n",
        "    self.linearlayer1=nn.Linear(self.d_model,self.hidlayer)\n",
        "    self.linearlayer2=nn.Linear(self.hidlayer,self.d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.activation=nn.ReLU()\n",
        "\n",
        "  def forward(self,x):\n",
        "    l1=self.linearlayer1(x)\n",
        "    #print(f\"x after first linear layer: {x.size()}\")\n",
        "    l1=self.activation(l1)\n",
        "    #print(f\"x after activation: {l1.size()}\")\n",
        "    l1=self.dropout(l1)\n",
        "    #print(f\"x after dropout 1: {l1.size()}\")\n",
        "    out=self.linearlayer2(l1)\n",
        "    #print(f\"x after 2nd linear layer: {out.size()}\")\n",
        "    #drop out gen not aplpied after 1st layhers\n",
        "    out=self.dropout(out)\n",
        "    #print(f\"x after dropout 2: {out.size()}\")\n",
        "    return out\n",
        "\n",
        "#multhead attention\n",
        "class multihead_attention(nn.Module):\n",
        "    def __init__(self,dmodel,heads=1,masking=False):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.masking = masking\n",
        "        self.dmodel=dmodel\n",
        "        assert self.dmodel % self.heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.head_dim = self.dmodel // self.heads\n",
        "        self.wq = nn.Linear(self.dmodel, self.dmodel)\n",
        "        self.wk = nn.Linear(self.dmodel, self.dmodel)\n",
        "        self.wv = nn.Linear(self.dmodel, self.dmodel)\n",
        "        self.linearlayer=nn.Linear(self.dmodel,self.dmodel)\n",
        "\n",
        "        #print('heads =', self.heads)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        dk = torch.tensor(q.shape[-1], dtype=torch.float32)\n",
        "        scaled = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(dk)\n",
        "\n",
        "        if self.masking:\n",
        "            mask = torch.ones(q.shape[2], q.shape[2], device=q.device)\n",
        "            mask = torch.tril(mask)\n",
        "            mask[mask == 0] = -torch.inf\n",
        "            mask[mask == 1] = 0\n",
        "            scaled = scaled + mask\n",
        "\n",
        "        attention = torch.softmax(scaled, dim=-1)\n",
        "        scores = torch.matmul(attention, v)\n",
        "        return attention, scores\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        q = self.wq(x)\n",
        "        k = self.wk(x)\n",
        "        v = self.wv(x)\n",
        "        q = q.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        k = k.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        v = v.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attention, scores = self.scaled_dot_product_attention(q, k, v)\n",
        "        #print('scores init',scores.shape)\n",
        "        scores = scores.reshape(batch_size, sequence_length, self.heads *self.head_dim)# we can use self.dmodel as well as last arg\n",
        "        #print('scores shape',scores.shape)\n",
        "        out=self.linearlayer(scores)\n",
        "        #print('out',out.shape)\n",
        "        #print('projected shape',projected.shape)\n",
        "        #print()\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "#layer norm\n",
        "class CustomLayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape, epsilon=1e-5):\n",
        "        super(CustomLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate mean and std across the last dimension (features) for each sequence in the batch\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        x_normalized = (x - mean) / (std + self.epsilon)\n",
        "\n",
        "        # Apply gamma and beta, which are learned parameters for normalization\n",
        "        # The shape of gamma and beta should match the feature size\n",
        "        return self.gamma.unsqueeze(0).unsqueeze(0) * x_normalized + self.beta.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "class multihead_cross_attention(nn.Module):\n",
        "    def __init__(self, dmodel, masking=None, heads=1):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.masking = masking\n",
        "        assert dmodel % heads == 0, \"Embedding dimension must be divisible by num_heads\"\n",
        "\n",
        "        self.dmodel = dmodel\n",
        "        self.head_dim = self.dmodel // self.heads\n",
        "        self.wqc = nn.Linear(self.dmodel, self.dmodel)\n",
        "        self.wkc = nn.Linear(self.dmodel, self.dmodel)\n",
        "        self.wvc = nn.Linear(self.dmodel, self.dmodel)\n",
        "        self.linearlayer=nn.Linear(self.dmodel,self.dmodel)\n",
        "        #print('heads =', self.heads)\n",
        "\n",
        "    def scaled_dot_product_attention(self, q, k, v):\n",
        "        dk = torch.tensor(q.shape[-1], dtype=torch.float32)\n",
        "        scaled = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(dk)\n",
        "\n",
        "        if self.masking is not None:\n",
        "            mask = torch.ones(q.shape[2], q.shape[2], device=q.device)\n",
        "            mask = torch.tril(mask)\n",
        "            mask[mask == 0] = -torch.inf\n",
        "            mask[mask == 1] = 0\n",
        "            scaled = scaled + mask\n",
        "\n",
        "        attention = torch.softmax(scaled, dim=-1)\n",
        "        scores = torch.matmul(attention, v)\n",
        "        return attention, scores\n",
        "\n",
        "    def forward(self, x,y):\n",
        "        batch_size, sequence_length, input_dim = x.size()\n",
        "        # if y.size(0) != x.size(0):\n",
        "        #      pad_size = x.size(0) - y.size(0)\n",
        "        #      padding = torch.zeros(pad_size, y.size(1), y.size(2), device=y.device)\n",
        "        #      y = torch.cat([y, padding], dim=0)\n",
        "\n",
        "        #print('x and y shape cross attention',x.shape,y.shape)\n",
        "\n",
        "        # q and k from decoder\n",
        "        q = self.wqc(x)\n",
        "        k = self.wkc(y)\n",
        "        # v from decoder\n",
        "        v = self.wvc(y)\n",
        "        q = q.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        k = k.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        v = v.view(batch_size, sequence_length, self.heads, self.head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attention, scores = self.scaled_dot_product_attention(q, k, v)\n",
        "        #print('scores init',scores.shape)\n",
        "        scores = scores.reshape(batch_size, sequence_length, self.heads *self.head_dim)# we can use self.dmodel as well as last arg\n",
        "        #print('scores shape',scores.shape)\n",
        "        out=self.linearlayer(scores)\n",
        "        #print('out',out.shape)\n",
        "\n",
        "        #print('projected shape',projected.shape)\n",
        "        #print(type(out))\n",
        "        return out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9_4pnfgjd33",
        "outputId": "9642cbcf-b6ac-4883-d61a-07ebd7d71194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "engtoken tensor([[[ 3.2072, -4.1821],\n",
            "         [-1.6482, -0.8999],\n",
            "         [ 1.4268,  0.6317],\n",
            "         [-0.0947, -0.0738],\n",
            "         [ 1.0261,  0.4570]],\n",
            "\n",
            "        [[ 3.2072, -4.1821],\n",
            "         [ 0.7681, -1.5612],\n",
            "         [-0.9090,  1.2549],\n",
            "         [ 0.1845, -0.2415],\n",
            "         [ 1.0261,  0.4570]],\n",
            "\n",
            "        [[ 3.2072, -4.1821],\n",
            "         [ 1.5595, -0.7858],\n",
            "         [-1.2191,  0.8004],\n",
            "         [-0.0000,  0.0267],\n",
            "         [ 1.2430,  0.5002]]], grad_fn=<MulBackward0>)\n",
            "input tokenization of x\n",
            "torch.Size([1, 5])\n",
            "hintoken tensor([[[ 3.2072, -4.1821],\n",
            "         [-1.2375,  0.0000],\n",
            "         [-1.2191,  0.8004],\n",
            "         [-0.2262,  0.0267],\n",
            "         [ 1.2430,  0.5002]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# === Special Tokens ===\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PADDING_TOKEN = \"<PAD>\"\n",
        "eng_to_index = {\n",
        "    \"Browse\": 1, \"the\": 2, \"various\": 3, \"methods\": 4, \"of\": 5, \"current\": 6, \"accessible\": 7,\n",
        "    \"Hide\": 8, \"private\": 9, \"attributes\": 10, \"Method\": 11,\n",
        "    START_TOKEN: 12, END_TOKEN: 13, PADDING_TOKEN: 0  # Special tokens\n",
        "}\n",
        "hindi_to_index = {\n",
        "    \"इस\": 1, \"समय\": 2, \"जिसे\": 3, \"प्राप्त\": 4, \"किया\": 5, \"गया\": 6, \"हो\": 7, \"विभिन्न\": 8,\n",
        "    \"विधियों\": 9, \"में\": 10, \"विचरण\": 11, \"करें\": 12, \"निजी\": 13, \"गुणों\": 14, \"को\": 15,\n",
        "    \"छिपाएं\": 16, \"विधि\": 17,\n",
        "    START_TOKEN: 18, END_TOKEN: 19, PADDING_TOKEN: 0  # Special tokens\n",
        "}\n",
        "\n",
        "# === Example Bilingual Sentences (English ↔ Hindi) ===\n",
        "bilingual_batch = [\n",
        "    (\"Browse the various methods of the current accessible\", \"इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें\"),\n",
        "    (\"Hide private attributes\", \"निजी गुणों को छिपाएं\"),\n",
        "    (\"Method\", \"विधि\")\n",
        "]\n",
        "\n",
        "# === Parameters ===\n",
        "max_seq_length = 5  # ✅ Limit to 5 words per sentence\n",
        "\n",
        "# === Separate English and Hindi Sentences ===\n",
        "english_sentences = [pair[0] for pair in bilingual_batch]\n",
        "hindi_sentences = [pair[1] for pair in bilingual_batch]\n",
        "\n",
        "d_model = 2\n",
        "batch_size = 3\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 5\n",
        "hn_vocab_size = len(hindi_to_index)\n",
        "eng_vocab_size = len(eng_to_index)\n",
        "#torch.manual_seed(2)\n",
        "tokenization=SentenceEmbedding(max_sequence_length, d_model, eng_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "engtoken=tokenization(english_sentences,start=True,end=True)\n",
        "print('engtoken',engtoken)\n",
        "hintoken=tokenization(['को'],start=True,end=True)\n",
        "print('hintoken',hintoken)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "TCs6Gf4_yVfs"
      },
      "outputs": [],
      "source": [
        "#encoder\n",
        "\n",
        "\n",
        "#encoder layer\n",
        "\n",
        "class encoderlayer(nn.Module):\n",
        "  def __init__(self,max_sequence_length,d_model,hidlayer,dropout,num_heads,masking):\n",
        "    super().__init__()\n",
        "    self.d_model,self.hidlayer,self.dropout,self.num_heads,self.masking=d_model,hidlayer,dropout,num_heads,masking\n",
        "    self.multihead_attention=multihead_attention(d_model,num_heads,masking,)\n",
        "    self.feedforward=feedforward(self.d_model,self.hidlayer,self.dropout)\n",
        "    self.layernorm1=CustomLayerNorm(self.d_model)\n",
        "    self.layernorm2=CustomLayerNorm(self.d_model)\n",
        "    self.max_sequence_length=max_sequence_length\n",
        "\n",
        "  def forward(self,x):\n",
        "    #mulihead\n",
        "    #print('--mulihead attention--')\n",
        "    mha=self.multihead_attention(x)\n",
        "    #print(mha.shape)\n",
        "    #layernorm\n",
        "    #print('--layer normalisation--')\n",
        "    ln1=self.layernorm1(mha+x)\n",
        "    #print(ln1.shape)\n",
        "    #feedforward\n",
        "    #print('--feedforward network--')\n",
        "    ff=self.feedforward(ln1)\n",
        "    #print(ff.shape)\n",
        "    #layernorm\n",
        "    #print('--layer normalisation--')\n",
        "    out=self.layernorm2(ff+ln1)\n",
        "    #print(out.shape)\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "class encoder(nn.Module):\n",
        "  #creating n layer of layers\n",
        "  def __init__(self,max_sequence_length,d_model,hidlayer,dropout,num_heads,nlayers,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN,\n",
        "               masking=False):\n",
        "    super().__init__()\n",
        "    #sequentially stack encoders\n",
        "    self.encoderembeddings=SentenceEmbedding(max_sequence_length,d_model,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.layers=nn.Sequential(*[encoderlayer(max_sequence_length,d_model,hidlayer,dropout,num_heads,masking) for _ in range(nlayers)])\n",
        "\n",
        "  def forward(self, x,start,end):\n",
        "    x=self.encoderembeddings(x,start,end)\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      #print(f'\\n------layer {i+1}----- ')\n",
        "\n",
        "      x = layer(x)\n",
        "      #print(f\"--Output after layer {i+1}--: {x.size()}\")  # Printing the size after each layer\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LJJRnA0c563",
        "outputId": "2c397364-243b-4992-be61-151ca5db5004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "Encoder shape torch.Size([3, 5, 4])\n",
            "Encoder output: tensor([[[ 0.1579, -0.1973, -1.1918,  1.2311],\n",
            "         [ 0.4406, -0.3767,  1.1217, -1.1856],\n",
            "         [ 0.7753, -0.2086,  0.7642, -1.3309],\n",
            "         [ 0.0760, -0.9095,  1.3706, -0.5371],\n",
            "         [ 0.3911,  0.9277, -1.4066,  0.0878]],\n",
            "\n",
            "        [[ 0.2206, -0.1991, -1.2173,  1.1957],\n",
            "         [ 0.5783, -1.0724,  1.0817, -0.5876],\n",
            "         [ 1.0823,  0.0043,  0.2435, -1.3301],\n",
            "         [-1.2650, -0.1862,  0.3305,  1.1207],\n",
            "         [ 0.6804,  0.7541, -1.4026, -0.0320]],\n",
            "\n",
            "        [[ 0.2339, -0.2646, -1.1836,  1.2142],\n",
            "         [-1.4265,  0.8345,  0.5120,  0.0801],\n",
            "         [ 0.3899,  0.6289, -1.4926,  0.4739],\n",
            "         [ 0.2198,  0.0759,  1.0567, -1.3525],\n",
            "         [ 0.3320, -0.0632,  1.0593, -1.3280]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "##encoder testing\n",
        "\n",
        "d_model = 4\n",
        "batch_size = 3\n",
        "ffn_hidden = 64\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_layers = 2\n",
        "max_sequence_length = 5\n",
        "hn_vocab_size = len(hindi_to_index)\n",
        "eng_vocab_size = len(eng_to_index)\n",
        "#torch.manual_seed(2)\n",
        "tokenization=SentenceEmbedding(max_sequence_length, d_model, eng_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "\n",
        "Encoder =encoder(\n",
        "    max_sequence_length=max_sequence_length,d_model=d_model,hidlayer=ffn_hidden\n",
        "    ,dropout=drop_prob,num_heads=num_heads,nlayers=num_layers,language_to_index=eng_to_index,\n",
        "    START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN,masking=False\n",
        ")\n",
        "Encoderout=Encoder(english_sentences,start=True,end=True)\n",
        "\n",
        "print('Encoder shape', Encoderout.shape)\n",
        "print(\"Encoder output:\", Encoderout)\n",
        "\n",
        "\n",
        "#perfect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Ft8WWGHQmhYG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "CDBmKEpzzUGa"
      },
      "outputs": [],
      "source": [
        "#decoder\n",
        "class decoderlayer(nn.Module):\n",
        "  def __init__(self,max_sequence_length,d_model,hidlayer,dropout,num_heads,masking):\n",
        "    super().__init__()\n",
        "    self.d_model,self.hidlayer,self.dropout,self.num_heads,self.masking=d_model,hidlayer,dropout,num_heads,masking\n",
        "    #self.PositionalEncoding=PositionalEncoding(self.d_model,self.input_dim) #sinencoding\n",
        "    self.multihead_attention=multihead_attention(self.d_model,self.num_heads,self.masking)\n",
        "    self.layernorm1=CustomLayerNorm(self.d_model)\n",
        "    self.cross_attention=multihead_cross_attention(d_model,num_heads)\n",
        "    self.layernorm2=CustomLayerNorm(self.d_model)\n",
        "    self.feedforward=feedforward(self.d_model,self.hidlayer,self.dropout)\n",
        "    self.layernorm3=CustomLayerNorm(self.d_model)\n",
        "    self.max_sequence_length=max_sequence_length\n",
        "\n",
        "\n",
        "  def forward(self,x,y):\n",
        "    if y.size(0) != x.size(0):\n",
        "             pad_size = x.size(0) - y.size(0)\n",
        "             padding = torch.zeros(pad_size, y.size(1), y.size(2), device=y.device)\n",
        "             y = torch.cat([y, padding], dim=0)\n",
        "    #rope\n",
        "    #print('---positional encoding--')\n",
        "    #mulihead\n",
        "    #print('--mulihead attention--')\n",
        "    mha=self.multihead_attention(y)\n",
        "    #print(mha.shape)\n",
        "    #layernorm1\n",
        "    #print('--layer normalisation 1--')\n",
        "    ln1=self.layernorm1(mha+y)\n",
        "    #print(ln1.shape)\n",
        "    #cross attention\n",
        "    #print('--cross mulihead attention--')\n",
        "    cmha=self.cross_attention(x,ln1)\n",
        "    #print(cmha.shape)\n",
        "     #layernorm2\n",
        "    #print('--layer normalisation 2--')\n",
        "    ln2=self.layernorm2(cmha+ln1)\n",
        "    #print(ln2.shape)\n",
        "\n",
        "    #feedforward\n",
        "    #print('--feedforward network--')\n",
        "    ff=self.feedforward(ln1)\n",
        "    #print(ff.shape)\n",
        "    #layernorm3\n",
        "    #print('--layer normalisation--')\n",
        "    out=self.layernorm3(ff+ln2)\n",
        "    #print(out.shape)\n",
        "    return out\n",
        "\n",
        "class decoder(nn.Module):\n",
        "  #creating n layer of layers\n",
        "  def __init__(self,max_sequence_length,d_model,hidlayer,dropout,num_heads,nlayers,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN,\n",
        "               masking=True):\n",
        "    super().__init__()\n",
        "    #sequentially stack encoders\n",
        "    self.decoderembeddings=SentenceEmbedding(max_sequence_length,d_model,language_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.layers=nn.Sequential(*[decoderlayer(max_sequence_length,d_model,hidlayer,dropout,num_heads,masking) for _ in range(nlayers)])\n",
        "\n",
        "  def forward(self, x,y,start,end):\n",
        "    y = self.decoderembeddings(y, start,end)\n",
        "    for i, layer in enumerate(self.layers):\n",
        "      #print(f'\\n------layer {i+1}----- ')\n",
        "      l = layer(x,y)\n",
        "      #print(f\"--Output after layer {i+1}--: {l.size()}\")  # Printing the size after each layer\n",
        "    return l\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UivYdbcRkxBg"
      },
      "outputs": [],
      "source": [
        "#testing decoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLejcbKhhc3H",
        "outputId": "e1f32839-1f8c-46ec-c23e-6ed16d92cf2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([2, 5])\n",
            "Decoder shape torch.Size([3, 5, 4])\n",
            "Decoder output: tensor([[[-0.4844,  1.4728, -0.7262, -0.2622],\n",
            "         [-0.5261, -0.3494,  1.4908, -0.6153],\n",
            "         [-1.3840,  0.0191,  0.4062,  0.9587],\n",
            "         [ 0.4869,  0.6061,  0.4017, -1.4947],\n",
            "         [ 0.9403,  0.6813, -1.2211, -0.4005]],\n",
            "\n",
            "        [[-0.4589,  1.4854, -0.6808, -0.3457],\n",
            "         [ 0.4829,  1.0541, -0.2814, -1.2556],\n",
            "         [-1.4016,  0.0429,  0.4442,  0.9145],\n",
            "         [ 0.8510,  0.8225, -1.1522, -0.5213],\n",
            "         [ 0.8380,  0.7663, -1.2624, -0.3419]],\n",
            "\n",
            "        [[-0.0169,  1.2998, -0.1478, -1.1351],\n",
            "         [ 0.0278,  1.3573, -0.3782, -1.0069],\n",
            "         [ 0.7810,  0.9422, -0.7682, -0.9550],\n",
            "         [ 1.4119, -0.2435, -0.2204, -0.9480],\n",
            "         [ 1.4385, -0.1304, -0.4837, -0.8244]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "d_model = 4\n",
        "batch_size = 3\n",
        "ffn_hidden = 64\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_layers = 2\n",
        "max_sequence_length = 5\n",
        "hn_vocab_size = len(hindi_to_index)\n",
        "eng_vocab_size = len(eng_to_index)\n",
        "#torch.manual_seed(2)\n",
        "\n",
        "tokenization=SentenceEmbedding(max_sequence_length, d_model, eng_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "\n",
        "Decoder = decoder(\n",
        "    max_sequence_length=max_sequence_length,d_model=d_model,hidlayer=ffn_hidden\n",
        "    ,dropout=drop_prob,num_heads=num_heads,nlayers=num_layers,language_to_index=hindi_to_index,\n",
        "    START_TOKEN=START_TOKEN,END_TOKEN=END_TOKEN,PADDING_TOKEN=PADDING_TOKEN,\n",
        "               masking=True\n",
        ")\n",
        "Decoderout=Decoder(Encoderout,['को','विधि'],start=True,end=True)\n",
        "\n",
        "print('Decoder shape', Decoderout.shape)\n",
        "print(\"Decoder output:\", Decoderout)\n",
        "\n",
        "\n",
        "#perfect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "GInr22u09lfv",
        "outputId": "c9120117-7c0e-40a4-fa82-c23607b4761b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder shape is  torch.Size([3, 5, 4])\n",
            "sample ['को', 'विधि']\n",
            "input tokenization of x\n",
            "torch.Size([2, 5])\n",
            "hintoken torch.Size([2, 5, 4])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[3, 5, 2, 2]' is invalid for input of size 40",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-99591e2f407a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhintoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultihead_cross_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoderout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Now batch size should match this swill show error eince matching shape is done at decoder so no issue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'multihead_cross_attention output shape is {out.shape} and output is {out}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mln1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCustomLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-566b2278a3b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwvc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[3, 5, 2, 2]' is invalid for input of size 40"
          ]
        }
      ],
      "source": [
        "#testing the combined class\n",
        "\n",
        "# === Special Tokens ===\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PADDING_TOKEN = \"<PAD>\"\n",
        "eng_to_index = {\n",
        "    \"Browse\": 2, \"the\": 1, \"various\": 4, \"methods\": 3, \"of\": 5, \"current\": 7, \"accessible\": 6,\n",
        "    \"Hide\": 11, \"private\": 10, \"attributes\": 12, \"Method\": 9,\n",
        "    START_TOKEN: 12, END_TOKEN: 13, PADDING_TOKEN: 0  # Special tokens\n",
        "}\n",
        "hindi_to_index = {\n",
        "    \"इस\": 1, \"समय\": 2, \"जिसे\": 3, \"प्राप्त\": 4, \"किया\": 5, \"गया\": 6, \"हो\": 7, \"विभिन्न\": 8,\n",
        "    \"विधियों\": 9, \"में\": 10, \"विचरण\": 11, \"करें\": 12, \"निजी\": 13, \"गुणों\": 14, \"को\": 15,\n",
        "    \"छिपाएं\": 16, \"विधि\": 17,\n",
        "    START_TOKEN: 18, END_TOKEN: 19, PADDING_TOKEN: 0  # Special tokens\n",
        "}\n",
        "\n",
        "# === Example Bilingual Sentences (English ↔ Hindi) ===\n",
        "bilingual_batch = [\n",
        "    (\"Browse the various methods of the current accessible\", \"इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें\"),\n",
        "    (\"Hide private attributes\", \"निजी गुणों को छिपाएं\"),\n",
        "    (\"Method\", \"विधि\")\n",
        "]\n",
        "\n",
        "# === Separate English and Hindi Sentences ===\n",
        "english_sentences = [pair[0] for pair in bilingual_batch]\n",
        "hindi_sentences = [pair[1] for pair in bilingual_batch]\n",
        "\n",
        "index_to_hindi = {v: k for k, v in hindi_to_index.items()}\n",
        "\n",
        "# === Parameters ===\n",
        "max_seq_length = 5  # ✅ Limit to 5 words per sentence\n",
        "\n",
        "# === Separate English and Hindi Sentences ===\n",
        "english_sentences = [pair[0] for pair in bilingual_batch]\n",
        "hindi_sentences = [pair[1] for pair in bilingual_batch]\n",
        "\n",
        "d_model = 4\n",
        "batch_size = 3\n",
        "ffn_hidden = 2048\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 5\n",
        "hn_vocab_size = len(hindi_to_index)\n",
        "eng_vocab_size = len(eng_to_index)\n",
        "#torch.manual_seed(2)\n",
        "tokenization=SentenceEmbedding(max_sequence_length, d_model, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "#engtoken=tokenization(english_sentences,start=True,end=True)\n",
        "#print('engtoken',engtoken)\n",
        "print('encoder shape is ',Encoderout.shape)\n",
        "sample=['को','विधि']#'विधि','इस समय जिसे प्राप्त किया गया हो']\n",
        "print('sample',sample)\n",
        "hintoken=tokenization(sample,start=True,end=True)\n",
        "print('hintoken',hintoken.shape)\n",
        "mha = multihead_attention(d_model, num_heads, masking=True)\n",
        "out = mha(hintoken)\n",
        "cat = multihead_cross_attention(d_model, heads=num_heads, masking=True)\n",
        "out = cat(Encoderout, y=out)  # Now batch size should match this swill show error eince matching shape is done at decoder so no issue\n",
        "print(f'multihead_cross_attention output shape is {out.shape} and output is {out}')\n",
        "ln1=CustomLayerNorm(d_model)\n",
        "print('ln1',ln1(out).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3G3u0W_ly-Tt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "wKyuqfKkzEd5"
      },
      "outputs": [],
      "source": [
        "#now the transformer class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ue6l3weSzmla"
      },
      "outputs": [],
      "source": [
        "#transformer class\n",
        "class mytransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               d_model,\n",
        "               hidlayer,\n",
        "               dropout,\n",
        "               num_heads,\n",
        "               nlayers,\n",
        "               lang_vocab_size,\n",
        "               english_to_index,\n",
        "               lang_to_index,\n",
        "               max_seq_len,\n",
        "               START_TOKEN,\n",
        "               END_TOKEN,\n",
        "               PADDING_TOKEN):\n",
        "\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.hidlayer=hidlayer\n",
        "    self.dropout=dropout\n",
        "    self.num_heads=num_heads\n",
        "    self.nlayers=nlayers\n",
        "    self.vocab_size=lang_vocab_size\n",
        "    self.english_to_index = english_to_index\n",
        "    self.lang_to_index = lang_to_index\n",
        "    self.max_sequence_length = max_seq_len\n",
        "    self.START_TOKEN = START_TOKEN\n",
        "    self.END_TOKEN = END_TOKEN\n",
        "    self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "\n",
        "    self.Encoder=encoder(self.max_sequence_length,self.d_model,self.hidlayer,self.dropout,self.num_heads,self.nlayers,\n",
        "                         self.english_to_index,self.START_TOKEN,self.END_TOKEN,self.PADDING_TOKEN,\n",
        "               )\n",
        "    self.Decoder=decoder(self.max_sequence_length,self.d_model,self.hidlayer,self.dropout,self.num_heads,self.nlayers,\n",
        "                         self.lang_to_index,self.START_TOKEN,self.END_TOKEN,self.PADDING_TOKEN,\n",
        "               masking=True)\n",
        "    self.linearlayer=nn.Linear(self.d_model,self.vocab_size)\n",
        "\n",
        "  def forward(self,english_sentences,hindi_sentences):\n",
        "    x=self.Encoder(english_sentences,start=False,end=False)\n",
        "    y=self.Decoder(x,hindi_sentences,start=True,end=True)\n",
        "    y=self.linearlayer(y)\n",
        "    print('output of linear layer',y.shape)\n",
        "    softmaxoutput=torch.softmax(y,dim=-1)\n",
        "    print('output of softmax layer',softmaxoutput.shape)\n",
        "    #return softmaxoutput\n",
        "    #predict next tokens\n",
        "    #predicted_tokens = torch.argmax(softmaxoutput, dim=-1)  # Shape: (3, 5)\n",
        "    # predict next token\n",
        "    predicted_next_token = torch.argmax(softmaxoutput[-1, -1, :])  # Shape: ()\n",
        "    return predicted_next_token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtHIzlHy7GAK",
        "outputId": "d4efcef2-ea95-4b70-aef8-a599ebb69f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "transformer shape torch.Size([])\n",
            "transformer output/predicted tokens: tensor(4)\n",
            "next Predicted word: प्राप्त\n"
          ]
        }
      ],
      "source": [
        "#next word prediction\n",
        "\n",
        "\n",
        "# === Special Tokens ===\n",
        "START_TOKEN = \"<START>\"\n",
        "END_TOKEN = \"<END>\"\n",
        "PADDING_TOKEN = \"<PAD>\"\n",
        "eng_to_index = {\n",
        "    \"Browse\": 2, \"the\": 1, \"various\": 4, \"methods\": 3, \"of\": 5, \"current\": 7, \"accessible\": 6,\n",
        "    \"Hide\": 11, \"private\": 10, \"attributes\": 12, \"Method\": 9,\n",
        "    START_TOKEN: 12, END_TOKEN: 13, PADDING_TOKEN: 0  # Special tokens\n",
        "}\n",
        "hindi_to_index = {\n",
        "    \"इस\": 1, \"समय\": 2, \"जिसे\": 3, \"प्राप्त\": 4, \"किया\": 5, \"गया\": 6, \"हो\": 7, \"विभिन्न\": 8,\n",
        "    \"विधियों\": 9, \"में\": 10, \"विचरण\": 11, \"करें\": 12, \"निजी\": 13, \"गुणों\": 14, \"को\": 15,\n",
        "    \"छिपाएं\": 16, \"विधि\": 17,\n",
        "    START_TOKEN: 18, END_TOKEN: 19, PADDING_TOKEN: 0  # Special tokens\n",
        "}\n",
        "\n",
        "# === Example Bilingual Sentences (English ↔ Hindi) ===\n",
        "bilingual_batch = [\n",
        "    (\"Browse the various methods of the current accessible\", \"इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें\"),\n",
        "    (\"Hide private attributes\", \"निजी गुणों को छिपाएं\"),\n",
        "    (\"Method\", \"विधि\")\n",
        "]\n",
        "\n",
        "# === Separate English and Hindi Sentences ===\n",
        "english_sentences = [pair[0] for pair in bilingual_batch]\n",
        "hindi_sentences = [pair[1] for pair in bilingual_batch]\n",
        "\n",
        "index_to_hindi = {v: k for k, v in hindi_to_index.items()}\n",
        "\n",
        "\n",
        "d_model = 64\n",
        "batch_size = 3\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 5\n",
        "hn_vocab_size = len(hindi_to_index)\n",
        "eng_vocab_size = len(eng_to_index)\n",
        "transformer=mytransformer(d_model,ffn_hidden,drop_prob,num_heads,num_layers,\n",
        "                          hn_vocab_size,eng_to_index,hindi_to_index,max_sequence_length,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "\n",
        "transformerout=transformer(english_sentences,hindi_sentences)\n",
        "print('transformer shape', transformerout.shape)\n",
        "print(\"transformer output/predicted tokens:\", transformerout)\n",
        "predicted_word = index_to_hindi.get(int(transformerout), \"<PAD>\")\n",
        "print(\"next Predicted word:\", predicted_word)\n",
        "\n",
        "\n",
        "#perfect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR4SdbItXFdt",
        "outputId": "3c53d291-409c-47f3-c485-14430e2c4bc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "mytransformer(\n",
              "  (Encoder): encoder(\n",
              "    (encoderembeddings): SentenceEmbedding(\n",
              "      (embedding): Embedding(14, 64)\n",
              "      (position_encoder): RoPEEmbedding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): Sequential(\n",
              "      (0): encoderlayer(\n",
              "        (multihead_attention): multihead_attention(\n",
              "          (wq): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wk): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wv): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (linearlayer): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (feedforward): feedforward(\n",
              "          (linearlayer1): Linear(in_features=64, out_features=2048, bias=True)\n",
              "          (linearlayer2): Linear(in_features=2048, out_features=64, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm1): CustomLayerNorm()\n",
              "        (layernorm2): CustomLayerNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (Decoder): decoder(\n",
              "    (decoderembeddings): SentenceEmbedding(\n",
              "      (embedding): Embedding(20, 64)\n",
              "      (position_encoder): RoPEEmbedding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): Sequential(\n",
              "      (0): decoderlayer(\n",
              "        (multihead_attention): multihead_attention(\n",
              "          (wq): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wk): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wv): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (linearlayer): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (layernorm1): CustomLayerNorm()\n",
              "        (cross_attention): multihead_cross_attention(\n",
              "          (wqc): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wkc): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wvc): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (linearlayer): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (layernorm2): CustomLayerNorm()\n",
              "        (feedforward): feedforward(\n",
              "          (linearlayer1): Linear(in_features=64, out_features=2048, bias=True)\n",
              "          (linearlayer2): Linear(in_features=2048, out_features=64, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm3): CustomLayerNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linearlayer): Linear(in_features=64, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCISLIG5pW5q",
        "outputId": "e75b54aa-72d0-4cbb-b2ff-41af9639ae09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['को']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample='को'\n",
        "sample.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb6aapGm3ZnF",
        "outputId": "4a4baf7e-881b-4b1c-fbcb-533c9138cf89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([2, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['गया']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#index_to_hindi.get(int(transformer(english_sentences,['इस', 'समय','समय समय'])), \"<PAD>\").split()\n",
        "#index_to_hindi.get(int(transformer(english_sentences,['समय'])), \"<PAD>\").split()\n",
        "index_to_hindi.get(int(transformer(english_sentences,['समय','इस'])), \"<PAD>\").split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoOoN3SurvMX",
        "outputId": "77a4402d-a8de-4700-e783-f53e0f5448cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([1, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([1, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "predicted_words <END> गया गया\n"
          ]
        }
      ],
      "source": [
        "numnextwords=2\n",
        "predicted_words=\"\"\n",
        "transformerout=transformer(english_sentences,hindi_sentences)\n",
        "predicted_word = index_to_hindi.get(int(transformerout), \"<PAD>\")\n",
        "predicted_words=predicted_word.split()\n",
        "for i in range(numnextwords):\n",
        "  transformerout=transformer(english_sentences,predicted_words[-1].split())\n",
        "  predicted_word = index_to_hindi.get(int(transformerout), \"<PAD>\")\n",
        "  predicted_words.append(predicted_word)\n",
        "\n",
        "print('predicted_words',' '.join(predicted_words))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hF6gQn0qzMOT"
      },
      "outputs": [],
      "source": [
        "# testing of characterwise/alphabet wise tokenization in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QmaBXw6nm0BW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qn3fgQQ8xdyZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = RoPEEmbedding(d_model)  # Ensure RoPEEmbedding is defined\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token=True, end_token=True):\n",
        "        def tokenize(sentence):\n",
        "            # Convert each character/token to its index\n",
        "            sentence_word_indices = [self.language_to_index.get(token, self.language_to_index[self.PADDING_TOKEN]) for token in list(sentence)]\n",
        "\n",
        "            if start_token:\n",
        "                sentence_word_indices.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indices.append(self.language_to_index[self.END_TOKEN])\n",
        "\n",
        "            # Ensure all sequences are exactly `max_sequence_length` long\n",
        "            sentence_word_indices = sentence_word_indices[:self.max_sequence_length]  # Truncate if too long\n",
        "            sentence_word_indices += [self.language_to_index[self.PADDING_TOKEN]] * (self.max_sequence_length - len(sentence_word_indices))  # Pad if too short\n",
        "\n",
        "            return torch.tensor(sentence_word_indices)\n",
        "\n",
        "        tokenized = [tokenize(sentence) for sentence in batch]\n",
        "        tokenized = torch.stack(tokenized)  # Convert list of tensors to a single tensor\n",
        "\n",
        "        return tokenized\n",
        "\n",
        "    def forward(self, x, start_token=True, end_token=True):\n",
        "        x = self.batch_tokenize(x, start_token, end_token)  # Ensure consistent function calls\n",
        "        x = self.embedding(x)  # Convert token indices to embeddings\n",
        "        pos = self.position_encoder(x)  # Apply positional encoding\n",
        "        x = self.dropout(x + pos)  # Apply dropout\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WExZY3YZr-kE"
      },
      "outputs": [],
      "source": [
        "\n",
        "START_TOKEN = '<start>'\n",
        "PADDING_TOKEN = '<padding>'\n",
        "END_TOKEN = '<end>'\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', '@',\n",
        "                      'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L',\n",
        "                      'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "                      'Y', 'Z', '[', '\\\\', ']', '^', '_', '`',\n",
        "                      'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                      'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                      'y', 'z', '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "\n",
        "\n",
        "hindi_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ँ', 'ं', 'ः',\n",
        "                    'अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ऋ', 'ऌ', 'ए', 'ऐ', 'ओ', 'औ',\n",
        "                    'क', 'ख', 'ग', 'घ', 'ङ', 'च', 'छ', 'ज', 'झ', 'ञ', 'ट', 'ठ', 'ड', 'ढ', 'ण',\n",
        "                    'त', 'थ', 'द', 'ध', 'न', 'प', 'फ', 'ब', 'भ', 'म', 'य', 'र', 'ल', 'व', 'श', 'ष', 'स', 'ह',\n",
        "                    '़', 'ऽ', 'ा', 'ि', 'ी', 'ु', 'ू', 'ृ', 'ॄ', 'ॅ', 'े', 'ै', 'ॉ', 'ो', 'ौ', '्', 'ॐ',\n",
        "                    '०', '१', '२', '३', '४', '५', '६', '७', '८', '९', PADDING_TOKEN, END_TOKEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "g-IrV2o7sCE8"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "index_to_hindi = {k:v for k,v in enumerate(hindi_vocabulary)}\n",
        "hindi_to_index = {v:k for k,v in enumerate(hindi_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "njRXbXV8sjvp"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Example Bilingual Sentences (English ↔ Hindi) ===\n",
        "bilingual_batch = [\n",
        "    (\"Browse the various methods of the current accessible\", \"इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें\"),\n",
        "    (\"Hide private attributes\", \"निजी गुणों को छिपाएं\"),\n",
        "    (\"Method\", \"विधि\")\n",
        "]\n",
        "\n",
        "# === Separate English and Hindi Sentences ===\n",
        "english_sentences = [pair[0] for pair in bilingual_batch]\n",
        "hindi_sentences = [pair[1] for pair in bilingual_batch]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2n_8wUeszf-",
        "outputId": "48ddadaf-bc53-4fea-de54-c0d9f8584537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें',\n",
              " 'निजी गुणों को छिपाएं',\n",
              " 'विधि']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hindi_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jXgAna0sG0E",
        "outputId": "3675926c-9550-4649-83d0-1acea8edb263"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 3\n",
            "Number of valid sentences: 3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "max_sequence_length = 100\n",
        "# to check if a token or character/alphabet ins engsen or hindi is present in about hindi/eng vocab pf charceter\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "#to check if engsend or hindisen each sent has max 200 charcers\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindi_sentences)):\n",
        "    hindi_sentence, english_sentence = hindi_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindi_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7coeuSr5uHDh",
        "outputId": "e0a34cc7-492a-4ff5-abde-a9e9cf603607"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 2]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valid_sentence_indicies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP9RlxSfh7wa",
        "outputId": "e4128a0d-8ea9-4e85-aa8b-2239837e0789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample ['को', 'विधि']\n",
            "hintoken torch.Size([2, 5, 4])\n"
          ]
        }
      ],
      "source": [
        "#testing the code\n",
        "\n",
        "d_model = 4\n",
        "batch_size = 3\n",
        "ffn_hidden = 2048\n",
        "num_heads = 2\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 5\n",
        "#torch.manual_seed(2)\n",
        "#engtokenization=SentenceEmbedding(max_sequence_length, d_model, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "#engtoken=tokenization(english_sentences,start=True,end=True)\n",
        "#print('engtoken',engtoken)\n",
        "#print('encoder shape is ',Encoderout.shape)\n",
        "sample=['को','विधि']#'विधि','इस समय जिसे प्राप्त किया गया हो']\n",
        "print('sample',sample)\n",
        "hintokenization=SentenceEmbedding(max_sequence_length, d_model, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "\n",
        "hintoken=hintokenization(sample,start_token=True,end_token=True)\n",
        "print('hintoken',hintoken.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_QxtlU3r_-W",
        "outputId": "5cbcb9af-fd84-4ea4-be47-0176d7194204"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Browse the various methods of the current accessible',\n",
              " 'Hide private attributes',\n",
              " 'Method']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFAez6JdiC7u",
        "outputId": "0df16b95-3edb-4807-ca03-2cf7e621b181"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['इस समय जिसे प्राप्त किया गया हो, उसकी विभिन्न विधियों (मेथड) में विचरण करें',\n",
              " 'निजी गुणों को छिपाएं',\n",
              " 'विधि']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hindi_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOmNXEvSlGXS",
        "outputId": "52820460-927e-47fa-b9fa-6b184a6a1586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([2, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['%']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "index_to_hindi.get(int(transformer(english_sentences,sample)), \"<PAD>\").split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d98EKTRxP7j",
        "outputId": "8044e45e-b189-4c0a-f266-3332b678adf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([1, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "input tokenization of x\n",
            "torch.Size([3, 5])\n",
            "input tokenization of x\n",
            "torch.Size([1, 5])\n",
            "output of linear layer torch.Size([3, 5, 20])\n",
            "output of softmax layer torch.Size([3, 5, 20])\n",
            "predicted_words 2 % %\n"
          ]
        }
      ],
      "source": [
        "numnextwords=2\n",
        "predicted_words=\"\"\n",
        "transformerout=transformer(english_sentences,hindi_sentences)\n",
        "predicted_word = index_to_hindi.get(int(transformerout), \"<PAD>\")\n",
        "predicted_words=predicted_word.split()\n",
        "for i in range(numnextwords):\n",
        "  transformerout=transformer(english_sentences,predicted_words[-1].split())\n",
        "  predicted_word = index_to_hindi.get(int(transformerout), \"<PAD>\")\n",
        "  predicted_words.append(predicted_word)\n",
        "\n",
        "print('predicted_words',' '.join(predicted_words))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "FGIRipM_A91_",
        "outputId": "95b288c0-9e07-47d0-8a19-873da47fadd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-4d516ddb-0b34-46e0-87ed-54eb96266a7f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Hindi</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Help!</td>\n",
              "      <td>बचाओ!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>उछलो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>कूदो.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Jump.</td>\n",
              "      <td>छलांग.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hello!</td>\n",
              "      <td>नमस्ते।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d516ddb-0b34-46e0-87ed-54eb96266a7f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4d516ddb-0b34-46e0-87ed-54eb96266a7f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4d516ddb-0b34-46e0-87ed-54eb96266a7f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-378e5aa5-fc9e-49f2-abfc-683209db23b0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-378e5aa5-fc9e-49f2-abfc-683209db23b0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-378e5aa5-fc9e-49f2-abfc-683209db23b0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  English    Hindi\n",
              "0   Help!    बचाओ!\n",
              "1   Jump.    उछलो.\n",
              "2   Jump.    कूदो.\n",
              "3   Jump.   छलांग.\n",
              "4  Hello!  नमस्ते।"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#testing with batches\n",
        "#break into batches\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the CSV file\n",
        "file_path = '/content/drive/MyDrive/ADeepLearning/hindi_english_parallel2.csv'\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Check the data\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "uAh7dkl3UVNQ"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvYu-o_tUVQv",
        "outputId": "e7328d3c-e483-47fe-d4f9-e2cd9cad8f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 10000\n",
            "Number of valid sentences: 3690\n"
          ]
        }
      ],
      "source": [
        "sample=df[:100000].copy()\n",
        "engsen=sample['English'].to_list()\n",
        "hindisen=sample['Hindi'].to_list()\n",
        "engsen1=engsen[:10000]\n",
        "hindisen1=hindisen[:10000]\n",
        "\n",
        "max_sequence_length = 100\n",
        "# to check if a token or character/alphabet ins engsen or hindi is present in about hindi/eng vocab pf charceter\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "#to check if engsend or hindisen each sent has max 200 charcers\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(hindisen1)):\n",
        "    hindi_sentence, english_sentence = hindisen1[index], engsen1[index]\n",
        "    if is_valid_length(hindi_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(hindi_sentence, hindi_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(hindisen1)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9t5zBn9Vl_W",
        "outputId": "3a3ccf8a-0ddd-4648-cd76-2b350e9ae27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total sentences: 10000\n",
            "Valid sentences: 760\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 50  # Maximum allowed characters\n",
        "min_sequence_length = 30  # Minimum required characters\n",
        "\n",
        "# Function to check if a sentence contains only valid tokens\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):  # Ensure unique characters are checked\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "# Function to check if sentence length is within min & max limits\n",
        "def is_valid_length(sentence, min_length, max_length):\n",
        "    sentence_length = len(sentence)  # No need to convert to list explicitly\n",
        "    return min_length <= sentence_length < max_length  # Ensuring it fits the range\n",
        "\n",
        "valid_sentence_indices = []\n",
        "for index in range(len(hindisen1)):\n",
        "    hindi_sentence, english_sentence = hindisen1[index], engsen1[index]\n",
        "\n",
        "    if (is_valid_length(hindi_sentence, min_sequence_length, max_sequence_length) and\n",
        "        is_valid_length(english_sentence, min_sequence_length, max_sequence_length) and\n",
        "        is_valid_tokens(hindi_sentence, hindi_vocabulary)):\n",
        "\n",
        "        valid_sentence_indices.append(index)\n",
        "\n",
        "print(f\"Total sentences: {len(hindisen1)}\")\n",
        "print(f\"Valid sentences: {len(valid_sentence_indices)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IkGl6r2ZVqPK"
      },
      "outputs": [],
      "source": [
        "hindisen1 = [hindisen1[i] for i in valid_sentence_indices]\n",
        "engsen1 = [engsen1[i] for i in valid_sentence_indices]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4IKgA_5Vtdc",
        "outputId": "b29742a0-2c87-4eed-d4f4-826dc5f2f53b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Are you interested in flowers?', 'Can you teach me how to steal?'), ('तुम्हें फूलों में दिलचस्पी है क्या?', 'क्या तुम मुझे चोरी करना सिखा सकते हो?')]\n",
            "[('Could I please use your phone?', 'Do you live with your parents?'), ('मैं आपका फ़ोन इस्तेमाल कर सकता हूँ क्या?', 'क्या तुम अपने मम्मी-पापा के साथ रहते हो?')]\n",
            "[('Do you remember what she said?', 'Excuse me, is this seat taken?'), ('तुम्हें याद है उसने क्या कहा था?', 'माफ़ कीजिएगा, यहाँ कोई बैठा हुआ है क्या?')]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "dataset = TextDataset(engsen1, hindisen1)\n",
        "#this code will create batches\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)\n",
        "\n",
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MGOmqwbV4lL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHQzJVM8UVT9",
        "outputId": "133cf9cd-5e6a-4efc-d377-218e42238ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "batch_num 1\n",
            "batch\n",
            "[('Are you interested in flowers?', 'Can you teach me how to steal?'), ('तुम्हें फूलों में दिलचस्पी है क्या?', 'क्या तुम मुझे चोरी करना सिखा सकते हो?')]\n",
            "engtoken torch.Size([2, 3, 2])\n",
            "hintoken torch.Size([2, 3, 2])\n",
            "\n",
            "batch_num 2\n",
            "batch\n",
            "[('Could I please use your phone?', 'Do you live with your parents?'), ('मैं आपका फ़ोन इस्तेमाल कर सकता हूँ क्या?', 'क्या तुम अपने मम्मी-पापा के साथ रहते हो?')]\n",
            "engtoken torch.Size([2, 3, 2])\n",
            "hintoken torch.Size([2, 3, 2])\n",
            "\n",
            "batch_num 3\n",
            "batch\n",
            "[('Do you remember what she said?', 'Excuse me, is this seat taken?'), ('तुम्हें याद है उसने क्या कहा था?', 'माफ़ कीजिएगा, यहाँ कोई बैठा हुआ है क्या?')]\n",
            "engtoken torch.Size([2, 3, 2])\n",
            "hintoken torch.Size([2, 3, 2])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#now combinig with sentence ebedding\n",
        "batch_size = 2\n",
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)\n",
        "d_model = 2\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 3\n",
        "torch.manual_seed(2)\n",
        "engtokenization=SentenceEmbedding(max_sequence_length, d_model, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "hintokenization=SentenceEmbedding(max_sequence_length, d_model, hindi_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "\n",
        "\n",
        "for batch_num, batch in enumerate(iterator):\n",
        "    print('\\nbatch_num',batch_num+1)\n",
        "    print('batch')\n",
        "    print(batch)\n",
        "    eng_batch, ln_batch = batch\n",
        "    engtoken=engtokenization(eng_batch,start_token=True,end_token=True)\n",
        "    print('engtoken',engtoken.shape)\n",
        "    hintoken=hintokenization(ln_batch,start_token=True,end_token=True)\n",
        "    print('hintoken',hintoken.shape)\n",
        "    if batch_num > 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIQCWRxFV9aE",
        "outputId": "59ca8a21-4519-486b-fb28-a65808d39f70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "mytransformer(\n",
              "  (Encoder): encoder(\n",
              "    (encoderembeddings): SentenceEmbedding(\n",
              "      (embedding): Embedding(14, 64)\n",
              "      (position_encoder): RoPEEmbedding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): Sequential(\n",
              "      (0): encoderlayer(\n",
              "        (multihead_attention): multihead_attention(\n",
              "          (wq): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wk): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wv): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (linearlayer): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (feedforward): feedforward(\n",
              "          (linearlayer1): Linear(in_features=64, out_features=2048, bias=True)\n",
              "          (linearlayer2): Linear(in_features=2048, out_features=64, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm1): CustomLayerNorm()\n",
              "        (layernorm2): CustomLayerNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (Decoder): decoder(\n",
              "    (decoderembeddings): SentenceEmbedding(\n",
              "      (embedding): Embedding(20, 64)\n",
              "      (position_encoder): RoPEEmbedding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): Sequential(\n",
              "      (0): decoderlayer(\n",
              "        (multihead_attention): multihead_attention(\n",
              "          (wq): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wk): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wv): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (linearlayer): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (layernorm1): CustomLayerNorm()\n",
              "        (cross_attention): multihead_cross_attention(\n",
              "          (wqc): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wkc): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (wvc): Linear(in_features=64, out_features=64, bias=True)\n",
              "          (linearlayer): Linear(in_features=64, out_features=64, bias=True)\n",
              "        )\n",
              "        (layernorm2): CustomLayerNorm()\n",
              "        (feedforward): feedforward(\n",
              "          (linearlayer1): Linear(in_features=64, out_features=2048, bias=True)\n",
              "          (linearlayer2): Linear(in_features=2048, out_features=64, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm3): CustomLayerNorm()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linearlayer): Linear(in_features=64, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HH1lJyEbaHj3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMe5OuYjaH9G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWc7i7GQW1c8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3PdpNi1Pl_y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from torch import nn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Positional_Encoding_in_Transformer_neural_networks.ipynb"
      ],
      "metadata": {
        "id": "UgNsS_MAQOD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sinoisal encoding\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "\n",
        "\n",
        "pe = PositionalEncoding(d_model=2, max_sequence_length=3)\n",
        "pe.forward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzqtU8XOQIBV",
        "outputId": "5fdf7788-6b72-4900-c08c-bac11dc669ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  1.0000],\n",
              "        [ 0.8415,  0.5403],\n",
              "        [ 0.9093, -0.4161]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for bathc\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Generate the positional encoding\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = torch.arange(self.max_sequence_length).reshape(self.max_sequence_length, 1)\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "\n",
        "        # Ensure the PE matches the batch size and sequence length\n",
        "        PE = PE.unsqueeze(0).expand(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return PE + x\n",
        "\n",
        "x = torch.randn(2, 3, 2)  # Example batch of sequences\n",
        "pe = PositionalEncoding(d_model=2, max_sequence_length=3)\n",
        "pe.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5goUyBiRa6l",
        "outputId": "0958c9f8-168c-4072-be3a-7c82de95f1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5655,  0.5307],\n",
              "         [ 2.0689,  1.7320],\n",
              "         [ 2.4299, -0.9372]],\n",
              "\n",
              "        [[-1.2400,  0.7311],\n",
              "         [-1.3401, -1.4478],\n",
              "         [ 0.3283, -1.6179]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rope encoding\n",
        "#rope\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RoPEEmbedding(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super().__init__()\n",
        "        assert embedding_dim % 2 == 0, \"Embedding dimension must be even for RoPE\"\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Rotary Position Embedding.\n",
        "\n",
        "        Args:\n",
        "        - x: Tensor of shape (batch_size, seq_len, embedding_dim)\n",
        "\n",
        "        Returns:\n",
        "        - Tensor with RoPE applied to the last two dimensions.\n",
        "        \"\"\"\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Generate position indices\n",
        "        position_ids = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
        "\n",
        "        # Compute the rotary angles\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, self.embedding_dim, 2, dtype=torch.float32, device=x.device) / self.embedding_dim))\n",
        "        angles = torch.einsum('i,j->ij', position_ids, freqs)\n",
        "\n",
        "        # Create the rotation matrix for sin and cos embeddings\n",
        "        sin = torch.sin(angles).repeat_interleave(2, dim=-1)\n",
        "        cos = torch.cos(angles).repeat_interleave(2, dim=-1)\n",
        "\n",
        "        # Apply rotation using cos and sin embeddings\n",
        "        x1 = x * cos + self.rotate_half(x) * sin\n",
        "        return x1+x\n",
        "\n",
        "\n",
        "\n",
        "    def rotate_half(self,x):\n",
        "        \"\"\"\n",
        "        Rotate the last dimension by swapping adjacent components and negating the correct ones.\n",
        "        \"\"\"\n",
        "        x1 = x[..., ::2]  # Elements at even positions: x1, x3, x5\n",
        "        x2 = x[..., 1::2]  # Elements at odd positions: x2, x4, x6\n",
        "        return torch.flatten(torch.stack([-x2, x1], dim=-1), start_dim=-2)  # Interleave and negate correctly\n",
        "\n",
        "\n",
        "batch_size = 2\n",
        "seq_len = 2\n",
        "embedding_dim = 2\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, embedding_dim)\n",
        "print(\"Input:\\n\", x)\n",
        "\n",
        "rope_layer = RoPEEmbedding(embedding_dim)\n",
        "rope_output = rope_layer(x)\n",
        "\n",
        "print(\"\\nOutput after RoPE:\\n\", rope_output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDBIjUntQLwf",
        "outputId": "1ceea41a-9c27-4bd0-9916-10e2c49465b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[[ 0.6181, -0.2385],\n",
            "         [ 0.0245, -0.4912]],\n",
            "\n",
            "        [[-0.0591, -1.5653],\n",
            "         [ 0.4258, -1.4818]]])\n",
            "\n",
            "Output after RoPE:\n",
            " tensor([[[ 1.2362, -0.4770],\n",
            "         [ 0.4511, -0.7360]],\n",
            "\n",
            "        [[-0.1183, -3.1305],\n",
            "         [ 1.9028, -1.9241]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sinuoisal pos emcioding basics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "max_sequence_length = 10\n",
        "d_model = 6\n",
        "\n",
        "#get even odd dims\n",
        "even_i = torch.arange(0, d_model, 2).float()\n",
        "odd_i=torch.arange(1, d_model, 2).float()\n",
        "even_i,odd_i\n"
      ],
      "metadata": {
        "id": "v8yERhOxRZ4H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d399a4-c704-439f-f670-5380ea4b7a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 2., 4.]), tensor([1., 3., 5.]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "edenominator = torch.pow(10000, (even_i)/d_model)\n",
        "odenominator=torch.pow(10000, (odd_i - 1)/d_model)\n",
        "edenominator,odenominator#both arre same so just use denom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czekR53jmAPX",
        "outputId": "6776dcbd-2aa2-4c8d-9836-aecbfcc0e425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  1.0000,  21.5443, 464.1590]),\n",
              " tensor([  1.0000,  21.5443, 464.1590]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "position = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length, 1)\n",
        "position"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKhqRdTumeTJ",
        "outputId": "9f9c1201-56dc-4ac8-e046-3cc2811d848e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [2.],\n",
              "        [3.],\n",
              "        [4.],\n",
              "        [5.],\n",
              "        [6.],\n",
              "        [7.],\n",
              "        [8.],\n",
              "        [9.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU9Q0SC2m3TD",
        "outputId": "6d5582e2-6b44-4f21-a843-a088c8487668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "denominator=edenominator\n",
        "denominator.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU45eqZfnSw8",
        "outputId": "4b14853c-d336-4f7f-8231-b286be1d3b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "denominator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX_2m3ifn9I3",
        "outputId": "909080f7-f3af-4a1b-bb9a-a01247a0abe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  1.0000,  21.5443, 464.1590])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "even_PE = torch.sin(position / denominator)\n",
        "odd_PE = torch.cos(position / denominator)\n",
        "even_PE,odd_PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES23pyfHnFYm",
        "outputId": "60ef2083-435b-46fa-fca0-ae69bbd987f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "         [ 0.8415,  0.0464,  0.0022],\n",
              "         [ 0.9093,  0.0927,  0.0043],\n",
              "         [ 0.1411,  0.1388,  0.0065],\n",
              "         [-0.7568,  0.1846,  0.0086],\n",
              "         [-0.9589,  0.2300,  0.0108],\n",
              "         [-0.2794,  0.2749,  0.0129],\n",
              "         [ 0.6570,  0.3192,  0.0151],\n",
              "         [ 0.9894,  0.3629,  0.0172],\n",
              "         [ 0.4121,  0.4057,  0.0194]]),\n",
              " tensor([[ 1.0000,  1.0000,  1.0000],\n",
              "         [ 0.5403,  0.9989,  1.0000],\n",
              "         [-0.4161,  0.9957,  1.0000],\n",
              "         [-0.9900,  0.9903,  1.0000],\n",
              "         [-0.6536,  0.9828,  1.0000],\n",
              "         [ 0.2837,  0.9732,  0.9999],\n",
              "         [ 0.9602,  0.9615,  0.9999],\n",
              "         [ 0.7539,  0.9477,  0.9999],\n",
              "         [-0.1455,  0.9318,  0.9999],\n",
              "         [-0.9111,  0.9140,  0.9998]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now we need to place one elem of even pe then odd pe then even pe then odd pe and so\n",
        "stacked=torch.stack([even_PE,odd_PE],dim=2)#dim=2 mean put odd pe in 2nd value or eve 2snd pos\n",
        "stacked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlX6Es_CnPNR",
        "outputId": "599280fe-eb22-4e44-ee06-507247927133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  1.0000],\n",
              "         [ 0.0000,  1.0000],\n",
              "         [ 0.0000,  1.0000]],\n",
              "\n",
              "        [[ 0.8415,  0.5403],\n",
              "         [ 0.0464,  0.9989],\n",
              "         [ 0.0022,  1.0000]],\n",
              "\n",
              "        [[ 0.9093, -0.4161],\n",
              "         [ 0.0927,  0.9957],\n",
              "         [ 0.0043,  1.0000]],\n",
              "\n",
              "        [[ 0.1411, -0.9900],\n",
              "         [ 0.1388,  0.9903],\n",
              "         [ 0.0065,  1.0000]],\n",
              "\n",
              "        [[-0.7568, -0.6536],\n",
              "         [ 0.1846,  0.9828],\n",
              "         [ 0.0086,  1.0000]],\n",
              "\n",
              "        [[-0.9589,  0.2837],\n",
              "         [ 0.2300,  0.9732],\n",
              "         [ 0.0108,  0.9999]],\n",
              "\n",
              "        [[-0.2794,  0.9602],\n",
              "         [ 0.2749,  0.9615],\n",
              "         [ 0.0129,  0.9999]],\n",
              "\n",
              "        [[ 0.6570,  0.7539],\n",
              "         [ 0.3192,  0.9477],\n",
              "         [ 0.0151,  0.9999]],\n",
              "\n",
              "        [[ 0.9894, -0.1455],\n",
              "         [ 0.3629,  0.9318],\n",
              "         [ 0.0172,  0.9999]],\n",
              "\n",
              "        [[ 0.4121, -0.9111],\n",
              "         [ 0.4057,  0.9140],\n",
              "         [ 0.0194,  0.9998]]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stacked.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebERVfJcpLn-",
        "outputId": "3c4824ce-ef66-4f82-ea0d-906a6fc9ace0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#now flatten is\n",
        "torch.flatten(stacked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mru5UjuupR0C",
        "outputId": "10eb9b97-d548-4306-fb54-8dd6b7b1007e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.8415,  0.5403,\n",
              "         0.0464,  0.9989,  0.0022,  1.0000,  0.9093, -0.4161,  0.0927,  0.9957,\n",
              "         0.0043,  1.0000,  0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000,\n",
              "        -0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000, -0.9589,  0.2837,\n",
              "         0.2300,  0.9732,  0.0108,  0.9999, -0.2794,  0.9602,  0.2749,  0.9615,\n",
              "         0.0129,  0.9999,  0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999,\n",
              "         0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999,  0.4121, -0.9111,\n",
              "         0.4057,  0.9140,  0.0194,  0.9998])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verify if\n",
        "even_PE,odd_PE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8MMKZSXpXRn",
        "outputId": "bfff67c9-3265-4275-de52-d062760d5cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000,  0.0000,  0.0000],\n",
              "         [ 0.8415,  0.0464,  0.0022],\n",
              "         [ 0.9093,  0.0927,  0.0043],\n",
              "         [ 0.1411,  0.1388,  0.0065],\n",
              "         [-0.7568,  0.1846,  0.0086],\n",
              "         [-0.9589,  0.2300,  0.0108],\n",
              "         [-0.2794,  0.2749,  0.0129],\n",
              "         [ 0.6570,  0.3192,  0.0151],\n",
              "         [ 0.9894,  0.3629,  0.0172],\n",
              "         [ 0.4121,  0.4057,  0.0194]]),\n",
              " tensor([[ 1.0000,  1.0000,  1.0000],\n",
              "         [ 0.5403,  0.9989,  1.0000],\n",
              "         [-0.4161,  0.9957,  1.0000],\n",
              "         [-0.9900,  0.9903,  1.0000],\n",
              "         [-0.6536,  0.9828,  1.0000],\n",
              "         [ 0.2837,  0.9732,  0.9999],\n",
              "         [ 0.9602,  0.9615,  0.9999],\n",
              "         [ 0.7539,  0.9477,  0.9999],\n",
              "         [-0.1455,  0.9318,  0.9999],\n",
              "         [-0.9111,  0.9140,  0.9998]]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#above done thats it!"
      ],
      "metadata": {
        "id": "klassBsopcY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#larnable pos embeds\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "class LearnablePositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_seq_length, d_model):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(max_seq_length, d_model)  # Learnable parameters here!\n",
        "\n",
        "    def forward(self, x):\n",
        "        position_ids = torch.arange(x.size(1), device=x.device)\n",
        "        return x + self.embedding(position_ids)"
      ],
      "metadata": {
        "id": "TkOjtsjUQ3il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rotate_half1(x):\n",
        "    \"\"\"\n",
        "    Rotate the last dimension by swapping adjacent even and odd components\n",
        "    and negating the even ones according to RoPE specifications.\n",
        "    \"\"\"\n",
        "    x1 = x[..., ::2]  # x1, x3, x5 (even indices in zero-based indexing)\n",
        "    x2 = x[..., 1::2]  # x2, x4, x6 (odd indices)\n",
        "    return torch.cat([-x2, x1], dim=-1)  # Negate the odd group\n",
        "\n",
        "def rotate_half(x):\n",
        "    \"\"\"\n",
        "    Rotate the last dimension by swapping adjacent components and negating the correct ones.\n",
        "    \"\"\"\n",
        "    x1 = x[..., ::2]  # Elements at even positions: x1, x3, x5\n",
        "    x2 = x[..., 1::2]  # Elements at odd positions: x2, x4, x6\n",
        "    return torch.flatten(torch.stack([-x2, x1], dim=-1), start_dim=-2)  # Interleave and negate correctly\n",
        "\n",
        "\n",
        "torch.manual_seed(2)\n",
        "x=torch.randn(2,2,2)\n",
        "print('x',x)\n",
        "print('out',rotate_half(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YfvOeBeQ5FB",
        "outputId": "3af22bed-6641-410c-de7c-0b117e695e09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x tensor([[[ 0.3923, -0.2236],\n",
            "         [-0.3195, -1.2050]],\n",
            "\n",
            "        [[ 1.0445, -0.6332],\n",
            "         [ 0.5731,  0.5409]]])\n",
            "out tensor([[[ 0.2236,  0.3923],\n",
            "         [ 1.2050, -0.3195]],\n",
            "\n",
            "        [[ 0.6332,  1.0445],\n",
            "         [-0.5409,  0.5731]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mNCI0JY1FM9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}